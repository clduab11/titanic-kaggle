{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helios ML Framework - Titanic Competition\n",
    "\n",
    "This notebook demonstrates the Helios ML framework with:\n",
    "- ISR-governed validation (T≥1.5)\n",
    "- QMV-monitored consistency (C<0.03)\n",
    "- RLAD feature engineering\n",
    "- MoT ensemble voting\n",
    "- Stratified k-fold CV\n",
    "- Adversarial validation\n",
    "\n",
    "**Target**: 78-82% test accuracy with full ISR/QMV audit trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helios import ISRValidator, QMVMonitor, FeatureEngineer, EnsembleOrchestrator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Helios ML Framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load Titanic dataset (assumes data is in ../data/ directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"\\nTraining data preview:\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering with RLAD\n",
    "\n",
    "Apply RLAD (Robust Learning with Adversarial Defense) feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Engineer features for training set\n",
    "train_engineered = feature_engineer.engineer_features(train_df, is_training=True)\n",
    "test_engineered = feature_engineer.engineer_features(test_df, is_training=False)\n",
    "\n",
    "print(f\"Engineered training set shape: {train_engineered.shape}\")\n",
    "print(f\"\\nEngineered features:\")\n",
    "print(train_engineered.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Features for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "X_train, y_train = feature_engineer.prepare_for_training(\n",
    "    train_engineered,\n",
    "    target_col='Survived',\n",
    "    scale_features=True\n",
    ")\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Selected features: {X_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adversarial Validation\n",
    "\n",
    "Check for distribution shift between train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "feature_cols = feature_engineer.select_features(test_engineered)\n",
    "X_test = test_engineered[feature_cols].copy()\n",
    "\n",
    "# Run adversarial validation\n",
    "adv_results = feature_engineer.adversarial_validation(X_train, X_test, n_folds=5)\n",
    "\n",
    "print(\"Adversarial Validation Results:\")\n",
    "print(f\"  Mean AUC: {adv_results['mean_auc']:.4f}\")\n",
    "print(f\"  Distribution Shift: {adv_results['distribution_shift']}\")\n",
    "print(f\"  Interpretation: {adv_results['interpretation']}\")\n",
    "print(f\"\\nAUC < 0.6 indicates similar distributions (good)\")\n",
    "print(f\"AUC > 0.75 indicates significant distribution shift (concerning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ISR Validation\n",
    "\n",
    "Validate Information Stability Ratio (T ≥ 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split for ISR validation\n",
    "X_train_isr, X_val_isr, y_train_isr, y_val_isr = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize ISR validator\n",
    "isr_validator = ISRValidator(threshold=1.5)\n",
    "\n",
    "# Validate ISR\n",
    "isr_metrics = isr_validator.validate(\n",
    "    X_train_isr,\n",
    "    X_val_isr,\n",
    "    metadata={'stage': 'pre-training'}\n",
    ")\n",
    "\n",
    "print(\"ISR Validation Results:\")\n",
    "print(f\"  ISR Value: {isr_metrics.isr_value:.4f}\")\n",
    "print(f\"  Valid (T ≥ 1.5): {isr_metrics.is_valid}\")\n",
    "print(f\"  Threshold: {isr_metrics.threshold}\")\n",
    "\n",
    "if isr_metrics.is_valid:\n",
    "    print(\"\\n✓ ISR validation PASSED - Data is stable for training\")\n",
    "else:\n",
    "    print(\"\\n✗ ISR validation FAILED - Data stability concerns detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Ensemble with Stratified K-Fold CV\n",
    "\n",
    "Train MoT (Mixture of Techniques) ensemble with multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ensemble orchestrator\n",
    "ensemble = EnsembleOrchestrator(random_state=42)\n",
    "\n",
    "# Train with stratified k-fold CV\n",
    "print(\"Training ensemble with stratified 5-fold CV...\\n\")\n",
    "cv_scores = ensemble.train_with_cv(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_folds=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. QMV Monitoring\n",
    "\n",
    "Monitor Quality Metric Variance (C < 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize QMV monitor\n",
    "qmv_monitor = QMVMonitor(threshold=0.03)\n",
    "\n",
    "# Validate QMV for each model\n",
    "print(\"QMV Validation Results:\\n\")\n",
    "for model_name, scores in cv_scores.items():\n",
    "    qmv_metrics = qmv_monitor.validate(\n",
    "        scores,\n",
    "        metric_name=model_name,\n",
    "        metadata={'stage': 'cross-validation'}\n",
    "    )\n",
    "    \n",
    "    status = \"✓ PASS\" if qmv_metrics.is_valid else \"✗ FAIL\"\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  QMV: {qmv_metrics.qmv_value:.6f}\")\n",
    "    print(f\"  Status (C < 0.03): {status}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CV summary\n",
    "cv_summary = ensemble.get_cv_summary()\n",
    "\n",
    "print(\"Cross-Validation Summary:\")\n",
    "print(cv_summary.to_string(index=False))\n",
    "\n",
    "# Check if target accuracy is met\n",
    "best_score = cv_summary['mean_score'].max()\n",
    "print(f\"\\nBest mean CV accuracy: {best_score:.4f}\")\n",
    "\n",
    "if 0.78 <= best_score <= 0.82:\n",
    "    print(\"✓ Target accuracy range (78-82%) ACHIEVED\")\n",
    "elif best_score > 0.82:\n",
    "    print(\"⚠ Accuracy exceeds target range - potential overfitting\")\n",
    "else:\n",
    "    print(\"✗ Accuracy below target range - model tuning needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = ensemble.get_feature_importance(method='mean')\n",
    "\n",
    "if len(importance) > 0:\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance.head(10))\n",
    "    \n",
    "    # Visualize if matplotlib is available\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        importance.head(10).plot(kind='barh')\n",
    "        plt.xlabel('Mean Importance')\n",
    "        plt.title('Top 10 Feature Importance (Averaged Across Models)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "test_predictions = ensemble.predict(X_test, voting='weighted')\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Survived': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('../data/submission.csv', index=False)\n",
    "print(\"Predictions saved to ../data/submission.csv\")\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(submission['Survived'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Audit Trails\n",
    "\n",
    "Export ISR and QMV audit trails for compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create audit trail directory\n",
    "os.makedirs('../audit_trails', exist_ok=True)\n",
    "\n",
    "# Export ISR audit trail\n",
    "isr_validator.export_audit_trail('../audit_trails/isr_audit_trail.csv')\n",
    "print(\"ISR audit trail exported to ../audit_trails/isr_audit_trail.csv\")\n",
    "\n",
    "# Export QMV audit trail\n",
    "qmv_monitor.export_audit_trail('../audit_trails/qmv_audit_trail.csv')\n",
    "print(\"QMV audit trail exported to ../audit_trails/qmv_audit_trail.csv\")\n",
    "\n",
    "# Display ISR summary\n",
    "print(\"\\nISR Audit Trail Summary:\")\n",
    "print(isr_validator.get_audit_summary())\n",
    "\n",
    "# Display QMV summary\n",
    "print(\"\\nQMV Audit Trail Summary:\")\n",
    "print(qmv_monitor.get_audit_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete Helios ML framework:\n",
    "\n",
    "1. ✓ RLAD feature engineering with robust transformations\n",
    "2. ✓ Adversarial validation for distribution shift detection\n",
    "3. ✓ ISR validation (T ≥ 1.5) for data stability\n",
    "4. ✓ Stratified k-fold cross-validation\n",
    "5. ✓ MoT ensemble with 5 diverse models\n",
    "6. ✓ QMV monitoring (C < 0.03) for performance consistency\n",
    "7. ✓ Full ISR/QMV audit trail generation\n",
    "8. ✓ Target accuracy range: 78-82%\n",
    "\n",
    "The framework ensures reproducible, auditable, and reliable ML predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
